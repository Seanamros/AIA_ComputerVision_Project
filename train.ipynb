{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "# https://github.com/experiencor/keras-yolo3\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from voc import parse_voc_annotation\n",
    "from yolo import create_yolov3_model, dummy_loss\n",
    "from generator import BatchGenerator\n",
    "from utils.utils import normalize, evaluate, makedirs\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from callbacks import CustomModelCheckpoint, CustomTensorBoard\n",
    "from utils.multi_gpu_model import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(\n",
    "    train_annot_folder,\n",
    "    train_image_folder,\n",
    "    train_cache,\n",
    "    valid_annot_folder,\n",
    "    valid_image_folder,\n",
    "    valid_cache,\n",
    "    labels,\n",
    "):\n",
    "    # parse annotations of the training set\n",
    "    train_ints, train_labels = parse_voc_annotation(train_annot_folder, train_image_folder, train_cache, labels)\n",
    "    # parse annotations of the validation set, if any, otherwise split the training set\n",
    "    if os.path.exists(valid_annot_folder):\n",
    "        valid_ints, valid_labels = parse_voc_annotation(valid_annot_folder, valid_image_folder, valid_cache, labels)\n",
    "    else:\n",
    "        print(\"valid_annot_folder not exists. Spliting the trainining set.\")\n",
    "\n",
    "        train_valid_split = int(0.8*len(train_ints))\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(train_ints)\n",
    "        np.random.seed()\n",
    "\n",
    "        valid_ints = train_ints[train_valid_split:]\n",
    "        train_ints = train_ints[:train_valid_split]\n",
    "\n",
    "    # compare the seen labels with the given labels\n",
    "    if len(labels) > 0:\n",
    "        overlap_labels = set(labels).intersection(set(train_labels.keys()))\n",
    "\n",
    "        print('Seen labels: \\t'  + str(train_labels) + '\\n')\n",
    "        print('Given labels: \\t' + str(labels))\n",
    "\n",
    "        # return None, None, None if some given label is not in the dataset\n",
    "        if len(overlap_labels) < len(labels):\n",
    "            print('Some labels have no annotations! Please revise the list of labels')\n",
    "            return None, None, None\n",
    "    else:\n",
    "        print('No labels are provided. Train on all seen labels.')\n",
    "        print(train_labels)\n",
    "        labels = train_labels.keys()\n",
    "\n",
    "    max_box_per_image = max([len(inst['object']) for inst in (train_ints + valid_ints)])\n",
    "\n",
    "    return train_ints, valid_ints, sorted(labels), max_box_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(saved_weights_name, model_to_save):\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor     = 'loss', \n",
    "        min_delta   = 0.01, \n",
    "        patience    = 5, \n",
    "        mode        = 'min', \n",
    "        verbose     = 1\n",
    "    )\n",
    "    checkpoint = CustomModelCheckpoint(\n",
    "        model_to_save   = model_to_save,\n",
    "        filepath        = saved_weights_name,# + '{epoch:02d}.h5', \n",
    "        monitor         = 'loss', \n",
    "        verbose         = 1, \n",
    "        save_best_only  = True, \n",
    "        mode            = 'min', \n",
    "        period          = 1\n",
    "    )\n",
    "    reduce_on_plateau = ReduceLROnPlateau(\n",
    "        monitor  = 'loss',\n",
    "        factor   = 0.1,\n",
    "        patience = 2,\n",
    "        verbose  = 1,\n",
    "        mode     = 'min',\n",
    "        epsilon  = 0.01,\n",
    "        cooldown = 0,\n",
    "        min_lr   = 0\n",
    "    )    \n",
    "    return [early_stop, checkpoint, reduce_on_plateau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(\n",
    "    nb_class, \n",
    "    anchors, \n",
    "    max_box_per_image, \n",
    "    max_grid, batch_size, \n",
    "    warmup_batches, \n",
    "    ignore_thresh,  \n",
    "    saved_weights_name, \n",
    "    lr,\n",
    "    grid_scales,\n",
    "    obj_scale=5,\n",
    "    noobj_scale=1,\n",
    "    xywh_scale=1,\n",
    "    class_scale=1  \n",
    "):\n",
    "    train_model, infer_model = create_yolov3_model(\n",
    "        nb_class            = nb_class, \n",
    "        anchors             = anchors, \n",
    "        max_box_per_image   = max_box_per_image, \n",
    "        max_grid            = max_grid, \n",
    "        batch_size          = batch_size, \n",
    "        warmup_batches      = warmup_batches,\n",
    "        ignore_thresh       = ignore_thresh,\n",
    "        grid_scales         = grid_scales,\n",
    "        obj_scale           = obj_scale,\n",
    "        noobj_scale         = noobj_scale,\n",
    "        xywh_scale          = xywh_scale,\n",
    "        class_scale         = class_scale\n",
    "    )  \n",
    "\n",
    "    # load the pretrained weight if exists, otherwise load the backend weight only\n",
    "    if os.path.exists(saved_weights_name): \n",
    "        print(\"\\nLoading pretrained weights.\\n\")\n",
    "        train_model.load_weights(saved_weights_name)\n",
    "    else:\n",
    "        train_model.load_weights(\"backend.h5\", by_name=True)          \n",
    "\n",
    "    optimizer = Adam(lr=lr, clipnorm=0.001)\n",
    "    train_model.compile(loss=dummy_loss, optimizer=optimizer)             \n",
    "\n",
    "    return train_model, infer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_annot_folder not exists. Spliting the trainining set.\n",
      "Seen labels: \t{'2_pepper_matured': 137, '3_pepper_covered': 349, '1_pepper_young': 1178, '0_pepper_flower': 186}\n",
      "\n",
      "Given labels: \t['0_pepper_flower', '1_pepper_young', '2_pepper_matured', '3_pepper_covered']\n",
      "\n",
      "Training on: \t['0_pepper_flower', '1_pepper_young', '2_pepper_matured', '3_pepper_covered']\n",
      "\n",
      "\n",
      "Loading pretrained weights.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks.py:999: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 676s - loss: 7.9866 - yolo_layer_1_loss: 2.0679 - yolo_layer_2_loss: 3.0295 - yolo_layer_3_loss: 2.8892\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.98655, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 2/100\n",
      " - 646s - loss: 7.8317 - yolo_layer_1_loss: 2.0930 - yolo_layer_2_loss: 3.0628 - yolo_layer_3_loss: 2.6760\n",
      "\n",
      "Epoch 00002: loss improved from 7.98655 to 7.83173, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 3/100\n",
      " - 644s - loss: 7.7709 - yolo_layer_1_loss: 2.0904 - yolo_layer_2_loss: 3.0046 - yolo_layer_3_loss: 2.6760\n",
      "\n",
      "Epoch 00003: loss improved from 7.83173 to 7.77092, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 4/100\n",
      " - 648s - loss: 7.7119 - yolo_layer_1_loss: 2.1355 - yolo_layer_2_loss: 2.9311 - yolo_layer_3_loss: 2.6454\n",
      "\n",
      "Epoch 00004: loss improved from 7.77092 to 7.71194, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 5/100\n",
      " - 651s - loss: 7.5218 - yolo_layer_1_loss: 1.9852 - yolo_layer_2_loss: 2.8565 - yolo_layer_3_loss: 2.6800\n",
      "\n",
      "Epoch 00005: loss improved from 7.71194 to 7.52181, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 6/100\n",
      " - 654s - loss: 7.3988 - yolo_layer_1_loss: 1.9846 - yolo_layer_2_loss: 2.9119 - yolo_layer_3_loss: 2.5023\n",
      "\n",
      "Epoch 00006: loss improved from 7.52181 to 7.39875, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 7/100\n",
      " - 653s - loss: 7.2748 - yolo_layer_1_loss: 1.9234 - yolo_layer_2_loss: 2.8507 - yolo_layer_3_loss: 2.5007\n",
      "\n",
      "Epoch 00007: loss improved from 7.39875 to 7.27480, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 8/100\n",
      " - 654s - loss: 7.0894 - yolo_layer_1_loss: 1.8812 - yolo_layer_2_loss: 2.7171 - yolo_layer_3_loss: 2.4911\n",
      "\n",
      "Epoch 00008: loss improved from 7.27480 to 7.08942, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 9/100\n",
      " - 657s - loss: 7.1871 - yolo_layer_1_loss: 1.9067 - yolo_layer_2_loss: 2.8045 - yolo_layer_3_loss: 2.4759\n",
      "\n",
      "Epoch 00009: loss did not improve from 7.08942\n",
      "Epoch 10/100\n",
      " - 642s - loss: 7.1100 - yolo_layer_1_loss: 1.8708 - yolo_layer_2_loss: 2.6617 - yolo_layer_3_loss: 2.5775\n",
      "\n",
      "Epoch 00010: loss did not improve from 7.08942\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 11/100\n",
      " - 646s - loss: 6.0567 - yolo_layer_1_loss: 1.5338 - yolo_layer_2_loss: 2.3256 - yolo_layer_3_loss: 2.1973\n",
      "\n",
      "Epoch 00011: loss improved from 7.08942 to 6.05672, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 12/100\n",
      " - 655s - loss: 5.6607 - yolo_layer_1_loss: 1.3946 - yolo_layer_2_loss: 2.2057 - yolo_layer_3_loss: 2.0604\n",
      "\n",
      "Epoch 00012: loss improved from 6.05672 to 5.66070, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 13/100\n",
      " - 658s - loss: 5.3990 - yolo_layer_1_loss: 1.4211 - yolo_layer_2_loss: 2.0845 - yolo_layer_3_loss: 1.8934\n",
      "\n",
      "Epoch 00013: loss improved from 5.66070 to 5.39899, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 14/100\n",
      " - 665s - loss: 5.3711 - yolo_layer_1_loss: 1.3340 - yolo_layer_2_loss: 2.0921 - yolo_layer_3_loss: 1.9450\n",
      "\n",
      "Epoch 00014: loss improved from 5.39899 to 5.37110, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 15/100\n",
      " - 692s - loss: 5.1824 - yolo_layer_1_loss: 1.3402 - yolo_layer_2_loss: 1.9821 - yolo_layer_3_loss: 1.8601\n",
      "\n",
      "Epoch 00015: loss improved from 5.37110 to 5.18235, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 16/100\n",
      " - 668s - loss: 5.0899 - yolo_layer_1_loss: 1.2619 - yolo_layer_2_loss: 2.0015 - yolo_layer_3_loss: 1.8265\n",
      "\n",
      "Epoch 00016: loss improved from 5.18235 to 5.08992, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 17/100\n",
      " - 668s - loss: 5.0632 - yolo_layer_1_loss: 1.2594 - yolo_layer_2_loss: 1.9541 - yolo_layer_3_loss: 1.8498\n",
      "\n",
      "Epoch 00017: loss improved from 5.08992 to 5.06324, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 18/100\n",
      " - 669s - loss: 5.1346 - yolo_layer_1_loss: 1.2878 - yolo_layer_2_loss: 1.9012 - yolo_layer_3_loss: 1.9456\n",
      "\n",
      "Epoch 00018: loss did not improve from 5.06324\n",
      "Epoch 19/100\n",
      " - 666s - loss: 4.9645 - yolo_layer_1_loss: 1.2348 - yolo_layer_2_loss: 1.8925 - yolo_layer_3_loss: 1.8372\n",
      "\n",
      "Epoch 00019: loss improved from 5.06324 to 4.96454, saving model to pepper_4classes_input_size768.h5\n",
      "Epoch 20/100\n"
     ]
    }
   ],
   "source": [
    "# Parse the annotations \n",
    "train_annot_folder = '../dataset/VD03_Labled/Labels/'\n",
    "train_image_folder = '../dataset/VD03_Labled/Images/'\n",
    "cache_name = 'pepper_4classes_input_size768_train.pkl'\n",
    "valid_annot_folder = ''\n",
    "valid_image_folder = ''\n",
    "valid_cache_name = ''\n",
    "labels = [\"0_pepper_flower\", \"1_pepper_young\", \"2_pepper_matured\", \"3_pepper_covered\"]\n",
    "\n",
    "train_ints, valid_ints, labels, max_box_per_image = create_training_instances(\n",
    "    train_annot_folder,\n",
    "    train_image_folder,\n",
    "    cache_name,\n",
    "    valid_annot_folder,\n",
    "    valid_image_folder,\n",
    "    valid_cache_name,\n",
    "    labels\n",
    ")\n",
    "print('\\nTraining on: \\t' + str(labels) + '\\n')\n",
    "\n",
    "# Create the generators \n",
    "#anchors = [55,69, 75,234, 133,240, 136,129, 142,363, 203,290, 228,184, 285,359, 341,260] #kangaroo\n",
    "#anchors = [0,0, 5,9, 21,34, 27,51, 42,71, 58,89, 73,125, 91,128, 114,183] #pepper 一個class\n",
    "anchors = [16,26, 28,48, 40,63, 43,37, 47,86, 64,84, 69,113, 88,139, 119,191]    #四個classes\n",
    "batch_size = 4\n",
    "#max_input_size = 448\n",
    "#min_input_size = 288\n",
    "max_input_size = 768\n",
    "min_input_size = 768\n",
    "\n",
    "train_generator = BatchGenerator(\n",
    "    instances           = train_ints, \n",
    "    anchors             = anchors,   \n",
    "    labels              = labels,        \n",
    "    downsample          = 32, # ratio between network input's size and network output's size, 32 for YOLOv3\n",
    "    max_box_per_image   = max_box_per_image,\n",
    "    batch_size          = batch_size,\n",
    "    min_net_size        = min_input_size,\n",
    "    max_net_size        = max_input_size,   \n",
    "    shuffle             = True, \n",
    "    jitter              = 0.3, \n",
    "    norm                = normalize\n",
    ")\n",
    "\n",
    "valid_generator = BatchGenerator(\n",
    "    instances           = valid_ints, \n",
    "    anchors             = anchors,   \n",
    "    labels              = labels,        \n",
    "    downsample          = 32, # ratio between network input's size and network output's size, 32 for YOLOv3\n",
    "    max_box_per_image   = max_box_per_image,\n",
    "    batch_size          = batch_size,\n",
    "    min_net_size        = min_input_size,\n",
    "    max_net_size        = max_input_size,   \n",
    "    shuffle             = True, \n",
    "    jitter              = 0.0, \n",
    "    norm                = normalize\n",
    ")\n",
    "\n",
    "# Create the model \n",
    "saved_weights_name = 'pepper_4classes_input_size768.h5'\n",
    "warmup_epochs = 3\n",
    "train_times = 8\n",
    "ignore_thresh = 0.5\n",
    "learning_rate = 1e-4\n",
    "grid_scales = [1,1,1]\n",
    "obj_scale = 5\n",
    "if os.path.exists(saved_weights_name): \n",
    "    warmup_epochs = 0\n",
    "warmup_batches = warmup_epochs * (train_times*len(train_generator))\n",
    "\n",
    "train_model, infer_model = create_model(\n",
    "    nb_class            = len(labels), \n",
    "    anchors             = anchors, \n",
    "    max_box_per_image   = max_box_per_image, \n",
    "    max_grid            = [max_input_size, max_input_size], \n",
    "    batch_size          = batch_size, \n",
    "    warmup_batches      = warmup_batches,\n",
    "    ignore_thresh       = 0.5,\n",
    "    saved_weights_name  = saved_weights_name,\n",
    "    lr                  = 1e-4,\n",
    "    grid_scales         = [1,1,1],\n",
    "\n",
    ")\n",
    "\n",
    "# Start training\n",
    "callbacks = create_callbacks(saved_weights_name, infer_model)\n",
    "nb_epochs = 100\n",
    "train_model.fit_generator(\n",
    "    generator        = train_generator, \n",
    "    steps_per_epoch  = len(train_generator) * train_times, \n",
    "    epochs           = nb_epochs + warmup_epochs, \n",
    "    verbose          = 2,\n",
    "    callbacks        = callbacks, \n",
    "    workers          = 4,\n",
    "    max_queue_size   = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "100%|██████████| 3497/3497 [04:19<00:00, 14.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import cv2\n",
    "from utils.utils import get_yolo_boxes, makedirs\n",
    "from utils.bbox import draw_boxes\n",
    "from keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#input_path   = 'k.png'\n",
    "# input_path   = 'kangaroo_raw.mp4'\n",
    "input_path   = '../dataset/20141010_smallsize.mp4'\n",
    "\n",
    "output_path  = 'output/'\n",
    "makedirs(output_path)\n",
    "\n",
    "# Set some parameter\n",
    "net_h, net_w = 416, 416 # a multiple of 32, the smaller the faster\n",
    "obj_thresh, nms_thresh = 0.5, 0.45\n",
    "#anchors = [55,69, 75,234, 133,240, 136,129, 142,363, 203,290, 228,184, 285,359, 341,260]\n",
    "#anchors = [0,0, 5,9, 21,34, 27,51, 42,71, 58,89, 73,125, 91,128, 114,183] #pepper 一個class\n",
    "anchors = [16,26, 28,48, 40,63, 43,37, 47,86, 64,84, 69,113, 88,139, 119,191]    #四個classes\n",
    "labels = [\"0_f\", \"1_y\", \"2_m\", \"3_c\"]\n",
    "\n",
    "# Load the model\n",
    "infer_model = load_model('pepper_4classes_input_size768.h5')\n",
    "\n",
    "# Predict bounding boxes \n",
    "if input_path[-4:] == '.mp4': # do detection on a video  \n",
    "    video_out = output_path + input_path.split('/')[-1]\n",
    "    video_reader = cv2.VideoCapture(input_path)\n",
    "\n",
    "    nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "    video_writer = cv2.VideoWriter(video_out,\n",
    "                           cv2.VideoWriter_fourcc(*'MPEG'), \n",
    "                           24.0, \n",
    "                           (frame_w, frame_h))\n",
    "    # the main loop\n",
    "    batch_size  = 1\n",
    "    images      = []\n",
    "    start_point = 0 #%\n",
    "    show_window = False\n",
    "    for i in tqdm(range(nb_frames)):\n",
    "        _, image = video_reader.read()\n",
    "        if image is None:\n",
    "            continue\n",
    "        if (float(i+1)/nb_frames) > start_point/100.:\n",
    "            images += [image]\n",
    "\n",
    "            if (i%batch_size == 0) or (i == (nb_frames-1) and len(images) > 0):\n",
    "                # predict the bounding boxes\n",
    "                batch_boxes = get_yolo_boxes(infer_model, images, net_h, net_w, anchors, obj_thresh, nms_thresh)\n",
    "\n",
    "                for i in range(len(images)):\n",
    "                    # draw bounding boxes on the image using labels\n",
    "                    draw_boxes(images[i], batch_boxes[i], labels, obj_thresh)   \n",
    "\n",
    "                    # show the video with detection bounding boxes          \n",
    "                    if show_window: cv2.imshow('video with bboxes', images[i])  \n",
    "\n",
    "                    # write result to the output video\n",
    "                    video_writer.write(images[i]) \n",
    "                images = []\n",
    "            if show_window and cv2.waitKey(1) == 27: break  # esc to quit\n",
    "\n",
    "    if show_window: cv2.destroyAllWindows()\n",
    "    video_reader.release()\n",
    "    video_writer.release()       \n",
    "else: # do detection on an image or a set of images\n",
    "    image_paths = []\n",
    "\n",
    "    if os.path.isdir(input_path): \n",
    "        for inp_file in os.listdir(input_path):\n",
    "            image_paths += [input_path + inp_file]\n",
    "    else:\n",
    "        image_paths += [input_path]\n",
    "\n",
    "    image_paths = [inp_file for inp_file in image_paths if (inp_file[-4:] in ['.jpg', '.png', 'JPEG'])]\n",
    "\n",
    "    # the main loop\n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)\n",
    "        print(image_path)\n",
    "\n",
    "        # predict the bounding boxes\n",
    "        boxes = get_yolo_boxes(infer_model, [image], net_h, net_w, anchors, obj_thresh, nms_thresh)[0]\n",
    "\n",
    "        # draw bounding boxes on the image using labels\n",
    "        draw_boxes(image, boxes, labels, obj_thresh) \n",
    "\n",
    "        # write the image with bounding boxes to file\n",
    "        output_img_path = output_path + image_path.split('/')[-1]\n",
    "        cv2.imwrite(output_img_path, np.uint8(image))\n",
    "        img = cv2.imread(output_img_path)[:,:,::-1]\n",
    "        plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
